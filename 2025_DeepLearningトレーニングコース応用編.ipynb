{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 応用編テキスト：モデルのチューニングと評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. モデルのハイパーパラメータチューニング\n",
    "ハイパーパラメータとは、モデルの学習プロセスや構造を制御する設定項目です。主なハイパーパラメータは以下の通りです：\n",
    "\n",
    "- 学習率: モデルがどれだけ早く収束するかを決定。\n",
    "- バッチサイズ: 一度に学習するデータの量。\n",
    "- エポック数: 全データセットを何回学習するか。\n",
    "- ニューロン数・層の数: ネットワークの表現力を決定。\n",
    "- ハイパーパラメータの調整は、性能に大きく影響します。小さな変更でも結果が変わるので、調整と評価を繰り返すことが重要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 過学習とその対策\n",
    "過学習とは、モデルが訓練データに過剰に適合してしまい、未知のデータに対して性能が低下する現象です。\n",
    "\n",
    "主な対策:\n",
    "\n",
    "- データ拡張: データをランダムに変形し、見かけ上のデータ量を増やす。\n",
    "- ドロップアウト: ニューラルネットワークの一部のノードをランダムに無効化して学習。\n",
    "- 正則化: モデルの重みにペナルティを課す（L2正則化など）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. モデル評価指標\n",
    "分類タスクの評価には以下の指標が役立ちます：\n",
    "\n",
    "- 正解率（Accuracy）: 正しく分類された割合。\n",
    "- 混同行列（Confusion Matrix）: 各クラスごとの予測結果の分布。\n",
    "- F1スコア: 再現率と適合率の調和平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価のために以下のツールを利用できます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例題1: ハイパーパラメータのチューニング\n",
    "以下の例では、学習率とバッチサイズを変更してMNISTモデルの性能を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3813\n",
      "Epoch [2/5], Loss: 0.2811\n",
      "Epoch [3/5], Loss: 0.1398\n",
      "Epoch [4/5], Loss: 0.2539\n",
      "Epoch [5/5], Loss: 0.0943\n",
      "Accuracy with learning rate 0.01: 95.20%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# データ前処理\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# モデル定義\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01  # 学習率\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# トレーニングループ\n",
    "for epoch in range(5):\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/5], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 評価\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy with learning rate {learning_rate}: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例題2: 過学習防止（ドロップアウトの利用）\n",
    "ドロップアウトを追加して、過学習を防止します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # ドロップアウト追加\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # ドロップアウト追加\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# ドロップアウトを使ったモデルを試してください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 追加課題\n",
    "学習率やバッチサイズを変えて、どの設定が最適か確認してください。\n",
    "\n",
    "ドロップアウトの割合（0.2 や 0.8）を変更してみましょう。\n",
    "\n",
    "混同行列を表示し、どの数字を誤認識しているか確認してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
